\documentclass{article}

\usepackage{multicol}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{float}
\usepackage{graphicx}
\usepackage{booktabs}

\title{Big Data For Engineers}
\author{Philip J. Hartout, Gian S. Hiltbrunner}
\date{June 2020}

\usepackage[sfdefault]{FiraSans}

\setcounter{tocdepth}{1}

\newminted[xml]{xml}{
    fontfamily = tt,
    fontsize = \normalsize,
    gobble = 1,
    samepage
}

\newminted[json]{json}{
    fontfamily = tt,
    fontsize = \normalsize,
    gobble = 1,
    samepage
}

\newminted[java]{java}{
    fontfamily = tt,
    fontsize = \normalsize,
    gobble = 1,
    samepage
}

\newminted[python]{python}{
    fontfamily = tt,
    fontsize = \normalsize,
    gobble = 1,
    samepage
}


\begin{document}
% \begin{multicols}{2}

\maketitle

\begin{center}
    \color{red}
    \begin{framed}
    \noindent Parts of the information provided within this document may be incomplete and/or incorrect. For corrections please submit a pull request to:
    \url{https://github.com/gianhiltbrunner/BigDataForEngineersSummary}
    \end{framed}
\end{center}

\tableofcontents

\section{Introduction}
\subsection{Are you capable of sketching the history of databases (ancient and modern) to a colleague in a few minutes?}

Early forms of databases include natural forms of information storage such as speaking, singing and writing. These forms became more complex in time when people performed simple accounting tasks using stone inscriptions and later on when books became widespread with the invention of the book press. The computer then finally enabled us to store and manage data on much larger scales.

In the 1960s file systems were introduced allowing simple forms of data managment, in the 1970s relational table databases such as SQL were introduced allowing systematic queries. In the 2000s other databases became more popular (NoSQL), these include key-value stores, triple stores, document stores and column stores.

\subsection{Do you know who Edgar Codd is?}

Edgar Codd was an English computer scientist who invented the relational model for database management, the theoretical basis for relational databases.

\subsection{Can you explain what Data Independence is, and why it is important?}

The actual data and application programs acting on the data remain unimpaired even if the storage representation or access methods are changed. This is important when we want to store small scale data on a single hard drive and store large scale data on cluster infrastructure but essentially want to access the data in the same manner.

\subsection{Do you know the rough conceptual difference between data, information, and knowledge?}

\begin{itemize}
    \item Data is conceived of as symbols or signs, representing stimuli or signals lacing any interpretation.
    \item Information is inferred from data", in the process of answering interrogative questions (e.g., "who", "what", "where", "how many", "when"), thereby making the data useful for "decisions and/or action".
    \item Knowledge represents information combined with understanding, capability and experience.
\end{itemize}


\subsection{Can you cite the five fundamental shapes of data, and how structured data can be characterized?}

Text, Tables, Trees, Graphs and Cubes

\begin{itemize}
    \item Data conforms to a data model and has easily identifiable structure
    \item Data is well organised so, Definition, Format and Meaning of data is explicitly known
    \item Data resides in fixed fields within a record or file
    \item Similar entities are grouped together to form relations or classes
    \item Entities in the same group have same attributes
    \item Easy to access and query, So data can be easily used by other programs
    \item Data elements are addressable, so efficient to analyse and process
\end{itemize}

\subsection{Can you briefly explain what a data model is?}

Underlying Questions

\begin{itemize}
    \item How does the data look like?
    \item What can be done with the data?
\end{itemize}

\noindent A data model is an abstract model that organizes elements of data and standardizes how they relate to one another and to the properties of real-world entities. For instance, a data model may specify that the data element representing a car be composed of a number of other elements which, in turn, represent the color and size of the car and define its owner.

\subsection{Do you know the 8 standard prefixes of the International System of Units (when the exponent in base 10 is a positive multiple of 3)?}

\begin{table}[H]
\begin{tabular}{|l|l|}
\hline
\textbf{Prefix} & \textbf{Digits} \\ \hline
kilo (k) & 1,000 (3 zeros) \\ \hline
Mega (M) & 1,000,000 (6 zeros) \\ \hline
Giga (G) & 1,000,000,000 (9 zeros) \\ \hline
Tera (T) & 1,000,000,000,000 (12 zeros) \\ \hline
Peta (P) & 1,000,000,000,000,000 (15 zeros) \\ \hline
Exa (E) & 1,000,000,000,000,000,000 (18 zeros) \\ \hline
Zetta (Z) & 1,000,000,000,000,000,000,000 (21 zeros) \\ \hline
Yotta (Y) & 1,000,000,000,000,000,000,000,000 (24 zeros) \\ \hline
\end{tabular}
\end{table}

\subsection{Can you list the main four technologies commonly referred to as NoSQL?}

\begin{itemize}
    \item Key-value stores
    \item Triple stores
    \item Column stores
    \item Document stores
\end{itemize}

\subsection{Do you know the three Vs?}


Big Data is defined by the three Vs.
\begin{itemize}
    \item \textbf{Volume}: High amount of data.
    \item \textbf{Variety}: Different types and structures.
    \item \textbf{Velocity}: High rate of data throughput.
\end{itemize}



\subsection{Can you briefly define capacity, throughput and latency? Do you know their typical units?}

\begin{itemize}
    \item \textbf{Capacity}: Volume of the stored data. (GB)
    \item \textbf{Throughput}: Speed of data transmission. (kbit/sec)
    \item \textbf{Latency}: Time until data is being received. (sec)
\end{itemize}

\subsection{Can you explain why and how the evolution of capacity, throughput and latency over the last few decades has influenced the design of modern databases?}

While the capacity has increased 200,000,000,000 fold, the throughput has increased only 10,000 fold and the latency only 8 fold.

\subsection{Can you name a few big players in the industry that accumulate and analyze massive amounts of data?}

Google, AWS (Amazon Web Services), Microsoft Azure and many more

\subsection{Do you know the difference between bit and byte?}

One byte is a collection of 8 bits where a bit represents the most basic form of information storage: 1 or 0.

\subsection{Can you name a few concrete examples that illustrate the various orders of magnitude of amounts of data?}





\section{Lessons learnt}
\subsection{Can you explain why it is important to take into consideration whether a use case is read-intensive, or write-intensive, or in-between?}


Write Intensive $\rightarrow$ \textbf{OLTP} (On-line Transaction Processing) is involved in the operation of a particular system. OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). The main emphasis for OLTP systems is put on very fast query processing, maintaining data integrity in multi-access environments and an effectiveness measured by number of transactions per second. In OLTP database there is detailed and current data, and schema used to store transactional databases is the entity model (usually 3NF). It involves queries accessing individual record like updating an email record in a database.\\\\

\noindent Read Intensive $\rightarrow$ \textbf{OLAP} (On-line Analytical Processing) deals with historical data or archival data. OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems a response time is an effectiveness measure. OLAP applications are widely used for data mining applications. In an OLAP database there is aggregated, historical data, stored in multi-dimensional schemas. Queries often access large amounts of data.

\subsection{Can you explain why normal forms are important?}

Normal forms are important to reduce data redundancy and improve data integrity. This is achieved by making sure that the data follows a set of rules corresponding to each of the normal forms.

\subsection{Can you describe the first normal form in simple terms?}

The first normal form expects the data to have a primary key ("id"), no repeating groups of entries (Not having columns such as "ChildName1" and "ChildName2") and atomicity of entries (No column such as "Children" with value e.g. "Max, Pia").

\subsection{Can you describe in simple terms (that is, without knowing the details of them) how higher normal forms (like Boyce-Codd) are related to joins?}

Higher order forms make sure that the data conforms to certain rules, thus when joining tables less conflicts occur.


\subsection{Can you explain why it is common, for large amounts of data, to drop several levels of normal form, and denormalize data instead?}

Denormalization is used to increase performance and scalability for large datasets. This leads to a increase in read performance while write performance suffers. This is due to the fact that relational databases need to perform many expensive JOIN operations when data is retrieved. These operations are prohibitively costly for very large datasets.

\begin{itemize}
    \item[+] Retrieving data is faster since we do fewer joins
    \item[+]Queries to retrieve can be simpler(and therefore less likely to have bugs), since we need to look at fewer tables.
    \item[-]Updates and inserts are more expensive.
    \item[-]Denormalization can make update and insert operations more complex.
    \item[-]Data may be inconsistent.
    \item[-]Data redundancy necessitates more storage.
\end{itemize}




\subsection{Can you give simple examples of denormalized data?}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{img/denormalization.png}
\end{figure}

\subsection{Can you explain what a declarative language is?}

Declarative languages such as SQL describe what needs to happen but do not care about how exactly something is happening. Thus we would tell the the interpreter to fetch an entry using SELECT but not really care about how this entry is selected. (e.g. looping over the entries, etc.)

\subsection{Can you explain what a functional language is?}

Functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that each return a value.

\subsection{Can you explain why it matters to design query languages that are declarative and functional?}

Declarative languages make sure that the we don't "reinvent the wheel" every time we create a query. Functional languages basically have similar benefits. They use functions to perform many operations such as map instead of looping over an array manually, thereby hiding away complexity and making code more readable.

\subsection{Can you describe the major relational algebra operators: select, project, group and aggregate, join, Cartesian product, union, intersection, etc?}

\begin{itemize}
    \item \textbf{SELECT}: Operation that subsets the data set given a condition. Such as SELECT all entries from the table Person whose age is larger than 34. ($\sigma_{\text{Age}\geq34}(\text{Person})$)
    \item \textbf{PROJECT}: Operation that picks a subset of all columns and drops duplicate entries. From table Person (Name, Age, Weight) project the columns Age and Weight to a new table while removing the Name column and excluding duplicate entries. ($ {\displaystyle \Pi _{\text{Age,Weight}}({\text{Person}})}$)
    \item \textbf{GROUP}: Operation that groups rows sharing the grouping property such that the groups can be aggregated in a subsequent operation. (e.g. SUM)
    \item \textbf{AGGREGATE}: Operation that summarizes multiple rows in a summarized form. Aggregate functions include: average, sum, median, standard deviation and many others.
    \item \textbf{JOIN}: Operation that unites two tables by the set of tuples that match. (e.g. key) \item \textbf{CARTESIAN PRODUCT}: Operation that creates a new table based on all possible combinations.
    \item \textbf{UNION}: Operation that combines tables from two SELECT operations.
    \item \textbf{INTERSECTION}: Operation that combines rows from tables that are present in both original data sets.
\end{itemize}

\subsection{Do you know what each letter stands for in ACID and CAP?}

\textbf{ACID}

\begin{itemize}
    \item Atomicity: Atomicity guarantees that each transaction is treated as a single "unit", which either succeeds completely, or fails completely.
    \item Consistency: Consistency ensures that a transaction can only bring the database from one valid state to another, maintaining database invariants.
    \item Isolation: Transactions are often executed concurrently (e.g., multiple transactions reading and writing to a table at the same time). Isolation ensures that concurrent execution of transactions leaves the database in the same state that would have been obtained if the transactions were executed sequentially.
    \item Durability: Durability guarantees that once a transaction has been committed, it will remain committed even in the case of a system failure (e.g., power outage or crash).
\end{itemize}

\noindent\textbf{CAP}

\begin{itemize}
    \item Consistency: Every copy of a data set has to be modified upon completion of a transaction. Note that this is not the same as consistency in the ACID model.
    \item Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write
    \item Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes
\end{itemize}

\subsection{Can you explain why, for large amounts of data, CAP becomes relevant over ACID?}

\noindent\textbf{ACID} addresses an individual node's data consistency\\
\textbf{CAP} addresses cluster-wide data consistency

\subsection{Do you know the names of the basic components of the tabular shape at an abstract level (table, row, column, primary key) as well as the names of the most common corresponding counterparts in the NoSQL world?}

\begin{table}[H]
\begin{tabular}{|l|l|}
\hline
\textbf{SQL} & \textbf{NoSQL} \\ \hline
database & database \\ \hline
table & collection \\ \hline
row & document \\ \hline
column & field \\ \hline
index & index \\ \hline
primary key & primary key \\ \hline
\end{tabular}
\end{table}

\subsection{Do you know the basic SQL constructs: SELECT FROM WHERE, GROUP, HAVING, JOIN, ORDER BY, LIMIT, OFFSET, as well as nested queries?}

\href{https://www.codecademy.com/articles/sql-commands}{Codecademy - SQL Commands}



\section{Object storage}
\subsection{Can you describe the limitations of traditional (local) file systems?}

Local file systems are limited in capacity by the hardware capacity, i.e. they cannot contain petabytes. Local filesystems can also have local hierarchical and does not allow flexible manipulation of it.

\subsection{Can you explain what object storage is?}

Object storage (also known as object-based storage) is a computer data storage architecture that manages data as objects, as opposed to other storage architectures like file systems which manages data as a file hierarchy, and block storage which manages data as blocks within sectors and tracks. (Wikipedia)

It uses a flat, simple and global key value model and flexible metadata to enable retention of a massive amount of unstructured data.

\subsection{Can you explain what the benefits of object storage are?}

It enables the retention of massive, and diverse amounts of data on commodity hardware, with easy key-value pair access, and flexible metadata. Scaling out in terms of hardware with this approach allows also to have more performance for a lower price.

\subsection{Can you contrast or relate object storage with block storage, a file system, and the key-value model?}

See wiki entry again: object storage (also known as object-based storage) is a computer data storage architecture that manages data as objects, as opposed to other storage architectures like file systems which manages data as a file hierarchy, and block storage which manages data as blocks within sectors and tracks. (Wikipedia)

\subsection{What is the CAP theorem?}

In theoretical computer science, the CAP theorem, also named Brewer's theorem after computer scientist Eric Brewer, states that it is impossible for a distributed data store to simultaneously provide more than two out of the following three guarantees:

\begin{itemize}
\item Consistency: Every read receives the most recent write or an error
\item Availability: Every request receives a (non-error) response, without the guarantee that it contains the most recent write
\item Partition tolerance: The system continues to operate despite an arbitrary number of messages being dropped (or delayed) by the network between nodes
\end{itemize}

When a network partition failure happens should we decide to

\begin{itemize}
\item Cancel the operation and thus decrease the availability but ensure consistency
\item Proceed with the operation and thus provide availability but risk inconsistency
\end{itemize}

The CAP theorem implies that in the presence of a network partition, one has to choose between consistency and availability. Note that consistency as defined in the CAP theorem is quite different from the consistency guaranteed in ACID database transactions.

\subsection{Can you explain the three different ways, on the physical level, to deal with more data (scale up, scale out, write better code)?}

\begin{itemize}
\item Scale up - get bigger machines, price increases exponentially
\item Scale out - get more commodity machines, price increases linearly
\item Write better code - avoids excessive processing overhead and storage requirements, price can be substantially lower.
\end{itemize}

\subsection{Can you explain why scaling out is less expensive than scaling up?}

Because commodity hardware manufacturing is streamlined its price increases linearly, whereas harder to make and hence more expensive machines which are obtained by scaling up increase the cost exponentially.

\subsection{Can you explain what aspects of the design of object storage enable scaling out and why?}

The architecture is simple and can easily be fragmented to be stored on commodity hardware.

\subsection{Do you know what a data centre is made of (racks, server nodes, storage nodes, etc.)?}

Racks: vertical stack of server nodes. Storage nodes are server nodes or VMs specialized for storage.

\subsection{Do you know the difference between storage, memory, CPU and network and how the three are paramount in a cluster?}

Yes.

\subsection{Do you know rough, typical numbers (per-node storage capacity, memory, number of cores, etc.)?}

Typical data center: 1,000-100,000 machines
Cores: 1-100 cores per machine
Storage: 1-20 TB local storage per server
RAM: 16-6TB.
Network bandwidth:1-100 Gb/s

\subsection{Do you know how storage and memory technologies (HDD, SDD and RAM) compare in terms of capacity and throughput?}

Capacity HDD>SDD>RAM
Throughput HDD<SDD<RAM

\subsection{Can you sketch the (key-value) data model behind object storage? Can you spot instances of the key-value model in several other technologies seen in the lecture?}

\begin{figure}[H]
  \centering
  \includegraphics[width=.5\textwidth]{img/key_value_object_storage.png}
  \caption{Key value schema and other features of object storage}
  \label{fig:keyvalueobjectstorage}
\end{figure}

Other instances: JSON, Azure cosmos DB key value paradigm, ...



\subsection{Do you know the difference between data and metadata?}

Metadata is data that provides information about other data. In other words, it is data about data.

\subsection{Do you know the order of magnitude that can be achieved in terms of number of objects, and object size?}

Object size ~5TB, 100 buckets per user by default.

\subsection{Can you name a few big players for cloud-based object storage (vendors, consumers)?}

Google, Microsoft, Amazon.

\subsection{Can you describe the features of S3 and Azure Blob Storage on a high level? Do you know what a bucket and object are? What block blob storage, append blob storage and page blob storage are and how they work?}

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    & S3 & Azure \\
    \midrule
    Object ID & Bucket + Object & Account + container + blob \\
    Object API & Blackbox & Block Append Page \\
    Limit & 5TB & 4.78TB (block), 195TB (append), 8TB (page)\\
    \bottomrule
  \end{tabular}
  \caption{Overall comparison Azure vs S3}
  \label{tab:azurevss3}
\end{table}


\subsection{Can you describe what the most important SLA (Service Level Agreement) parameters mean (e.g., latency, availability, durability) as well as their typical range?}

\begin{table}
  \centering
  \begin{tabular}{ll}
    \toprule
    SLA & Outage \\
    99\% & 4 days/year\\
    99.9\% & 9 hours/year\\
    99.99\% & 53 minutes/year\\
    99.999\% & 6 minutes/year\\
    99.9999\% & 32 seconds/year\\
    99.99999\% & 4 seconds/year\\
    \bottomrule
  \end{tabular}
  \label{tab:sla}
  \caption{SLA}
\end{table}

Durability is loss of 1 in $10^{11}$ objects a year.

Response time is <10ms in $99.9\%$.

\subsection{Can you describe a typical use case for object storage?}


\begin{itemize}
\item Disaster recovery — Backup and Archiving.
\item Static web site hosting and static content distribution.
\item Document Store and file sharing.
\item Big Data analytics.
\end{itemize}

\subsection{Can you explain what a REST API is, what resources and methods are?}


Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations. Other kinds of Web services, such as SOAP Web services, expose their own arbitrary sets of operations.

REST APIs use Uniform Resource Identifiers (URIs) to address resources. REST API designers should create URIs that convey a REST API's resource model to its potential client developers. When resources are named well, an API is intuitive and easy to use.

Methods include:
\begin{itemize}
\item HTTP GET
\item HTTP POST
\item HTTP PUT
\item HTTP DELETE
\item HTTP PATCH
\end{itemize}

Difference between PUT and PATCH: HTTP PATCH requests are to make partial update on a resource. If you see PUT requests also modify a resource entity, so to make more clear – PATCH method is the correct choice for partially updating an existing resource, and PUT should only be used if you’re replacing a resource in its entirety.

\subsection{Can you explain how an object storage API looks like (get and put)?}

You can address any combination of bucket and object name like so in S3:

\url{http://bucket.s3.amazonaws.com/object-name}

\subsection{Are you able to upload and download objects from S3 or Azure Blob Storage?}

Yes. This is also what one does when hosting a static website.



\section{Distributed file systems}
\subsection{Can you explain the difference between block storage and object storage?}

\textbf{Block storage} is organised as blocks, which emulate the type of behaviour seen in traditional disk or tape storage. Blocks are identified by an arbitrary and assigned identifier by which they may be stored and retrieved, but this has no obvious meaning in terms of files or documents. A filesystem must be applied on top of the block-level storage to map 'files' onto a sequence of blocks. (Replication or backups)\\
\textbf{Object store} or 'bucket store', such as Amazon S3 (Simple Storage Service) operate at a higher level of abstraction and are able to work with entities such as files, documents, images, videos or database records. Each object typically includes the data itself, a variable amount of metadata, and a globally unique identifier.

\subsection{Can you explain the difference between the (logical) key-value model and a file system?}

The key value model is typically built on top of an object storage system while a file system works on block storage.

\subsection{Can you contrast block storage and object storage in terms of maximum number of objects/files?}

\begin{itemize}
    \item \textbf{Key-Value Model}: Object Storage: Billions of sub-terabyte files. (Many small)
    \item \textbf{File System}: Block Storage: Millions of sub-petabyte files. (Few big)
\end{itemize}

\subsection{Can you contrast block storage and object storage in terms of the maximum size of objects/files?}

\begin{itemize}
    \item \textbf{Key-Value Model}: A terabyte
    \item \textbf{File System}: A petabyte
\end{itemize}

\subsection{Do you know the order of magnitude of a block size for a local file system and for a distributed file system? Can you explain the rationale behind them with respect to latency and throughput?}

\begin{itemize}
    \item \textbf{Local filesystem}: 4 kB
    \item \textbf{Distributed file system}: 64 MB - 128 MB
\end{itemize}

On a local system we have very high trougput and low latency, but on a cluster system the machines are connected over a network infrastructure which slows down transfer speeds. In order to make transfers more efficient larger blocks are used.

\subsection{Do you know where HDFS shines and why?}

\begin{itemize}
    \item Works with varied data such as text, XML, CSV and many more.
    \item Works on commodity hardware - Cheap
    \item High performance due to parallelization
    \item Fault-tolerant
    \item Scalable
\end{itemize}

\subsection{Do you know that HDFS files are updated by appending atomically and why?}

HDFS will always append to the last block and only one singe write operation is allowed at one time. Thus guaranteeing that there are no issues with concurrency.

\subsection{Do you know how HDFS performs in terms of throughput and latency?}

{\color{red} HDFS is optimized to access batches of data set quicker (high throughput), rather then particular records in that data set (low latency).}

\subsection{Do you know the main benefits of HDFS (commodity hardware, etc.)?}

See "Do you know where HDFS shines and why?".

\subsection{Can you contrast master-slave architectures to peer-to-peer architectures?}

n the master-slave model, one node is in charge (master). When there’s no single node with a special role in taking charge, you have a peer-to-peer distribution model.

\subsection{Can you explain the HDFS architecture, what a namenode is and what a datanode is, how blocks are replicated?}

HDFS works in a master-slave configuration. There is a master called the "Namenode" and multiple slaves called "Datanodes".

\begin{itemize}
    \item \textbf{Namenode}: Manages the file namespace, acess control file to block mapping and keeps track of the block locations.
    \item \textbf{Datanode}: System that stores blocks on the local disk.
    \item \textbf{Replication}: Blocks are replicated on multiple Datanodes for fault tolerance.
\end{itemize}

\subsection{Can you sketch how the various components communicate with each other (client, namenode, datanode)?}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{img/hdfsoverview.png}
\end{figure}

\begin{itemize}
    \item \textbf{Client Protocol}: Manages metadata operations, DataNode locations and block IDs.
    \item \textbf{DataNode Protocol}: Block operations such as replication managment and registration of the data node, heartbeat pings, block reports and block recieved confirmations. The DataNode always initiates the connection.
    \item \textbf{Data Transfer Protocol}: Data blocks are transferred from a DataNode to a client or back. The datanode then initiates replication pipelining.
\end{itemize}

{\color{red} Add precise data read and write protocol?}

\subsection{Can you point to the single points of failure of HDFS and explain how they can be addressed?}

The NameNode is a single point of failure.

We resolve this using:

\begin{itemize}
    \item \textbf{Persistance}: We store the inital namespace file and append an edit log with the changes that have been made.
    \item \textbf{Backup}: We back this file up.
\end{itemize}


\subsection{Can you explain how the namenode stores the file system namespace, in memory and on disk? In particular, can you explain how the namespace file and the edit log work together at startup time and how they get modified once the system is up and running?}
\subsection{Can you explain what a standby namenode is, what it is and what it is not?}
\subsection{Are you able to use the HDFS shell commands (creating directories, reading, uploading and downloading files, etc.)?}



\section{Syntax}
\subsection{Can you describe why syntax is relevant to data management and to Big Data, for all data shapes?}

Syntax is crucial for efficient data management, and should be adapted to the data type is represents, e.g. if the data is in first normal form (i.e. there is atomic integrity) then csv is good, but if there is nestedness you will probably have data duplication, in this case JSON is more appropriate and avoids redundancy.

\subsection{What are the system demands for normalized vs. denormalized data?}

Normalized data is write intensive, e.g. if there is nestedness. Denormalized data is read intensive.

\subsection{What do normalized vs. denormalized data avoid?}

Normalized: avoid update anomalies
Denormalized: avoids joins.

\subsection{Can you give examples of syntax for trees? For tables? (The two data shapes extensively covered in the lecture)}

CSV for tables and JSON for trees.


\subsection{Can you explain what well-formedness is with respect to syntax?}

well-formedness in the case of JSON for instance is when all the rules of the JSON syntax are respected by a JSON string.

\subsection{Can you list the XML basic building blocks (we covered document, element, attribute, text) and do you know what they look like?}

They look like this:
\begin{center}
\begin{listing}[!ht]
  \begin{xml}
    <element attribute="value">
    Some text
    </element>
  \end{xml}
  \caption[Test]{XML building blocks}
  \label{lst:test}
\end{listing}
\end{center}


\subsection{Can you tell whether a given XML document is well-formed, with a software (oXygen) as well as with your own eyes?}

Well formed json string

\begin{center}
\begin{listing}[!ht]
  \begin{json}
    {
      "tissue_directories": [
            "gtex/brain/brain_cerebellar_hemisphere.tpm.csv"
      ],
      "tissue_names": [
            "Brain Cerebellar Hemisphere"
      ],
      "sample_strategy": [
            "mean",
            "mean"
      ],
      "expression_selection": [
            [
                  "smaller",
                  100
            ],
            [
                  "smaller",
                  100
            ]
      ]
    }
  \end{json}
  \caption[Test]{JSON well formed}
  \label{lst:test}
\end{listing}
\end{center}

\begin{center}
\begin{listing}[!ht]
  \begin{json}

      "tissue_directories": [
            "gtex/brain/brain_cerebellar_hemisphere.tpm.csv"
      ],
      "tissue_names" [
            "Brain Cerebellar Hemisphere"
      ],
      sample_strategy: [
            "mean"
            "mean"
      ],
      "expression_selection": [
            [
                  "smaller",
                  100
            ],
            [
                  smaller,
                  100
            ]
      ]

  \end{json}
  \caption[Test]{JSON not well formed}
  \label{lst:test2}
\end{listing}
\end{center}

Another example of not well-formed JSON.

\subsection{Do you know the five fundamental pre-defined entities (used to escape characters: $\&$ < > ' ") and their syntax?}

See Table~\ref{tab:predefinedentitites}.

\begin{table}
  \centering
  \begin{tabular}{lllll}
    \toprule
    Name & Character & Unicode code point (decimal) & Standard & Name\\
    \midrule
    quot & " & U+0022 (34) & XML 1.0 & quotation mark\\
    amp  & \& & U+0026 (38) & XML 1.0 & ampersand\\
    apos & ' & U+0027 (39) & XML 1.0 & apostrophe \\
    lt & < & U+003C (60) & XML 1.0 & less-than sign\\
    gt & > & U+003E (62) & XML 1.0 & greater-than sign\\
    \bottomrule
  \end{tabular}
  \caption{Predefined entities. }
  \label{tab:predefinedentitites}
\end{table}

\subsection{Do you know when some characters must be escaped (e.g., < in text)?}

Characters must be escaped in text. In JSON, done with ``\textbackslash". In XML, follow the conve
ntions shown in Table~\ref{tab:predefinedentitites}.

\subsection{Can you list the JSON basic building blocks (object, array, string, number, boolean,null) and do you know what they look like?}

Shown in Listing~\ref{lst:test}.

\subsection{Can you tell whether a given JSON document is well-formed, also with your own eyes?}

Try with Listings~\ref{lst:test} and ~\ref{lst:test2}.

\subsection{What is the difference between validation and well-formedness}\label{diffwellformedvalidity}

validation comes after well-formedness, i.e. a well formed string is validated against a schema, which is a well-formed JSON string. Example of a schema is shown in:

\begin{center}
 \begin{listing}
   \begin{json}
     {"type": "object",
    "properties": {
        "tissue_directories": {
            "type": "array",
            "items": {
                "type": "string"
            }
        },
        "tissue_names": {
            "type": "array",
            "items": {
                "type": "string"
            }
        },
        "sample_strategy": {
            "type": "array",
            "items": {
                "oneOf": [
                    {
                        "type": "string",
                        "enum": ["mean", "max", "min", "median"
                        ]
                    },
                    {
                        "type": "number", "minimum": 0, "maximum": 1
                    }
                ]
            }
        },
        "expression_selection": {
            "type": "array",
            "items": {
                "type": "array",
                "items": [
                    {
                        "oneOf": [
                            {
                                "type": "string",
                                "enum": ["greater", "smaller"
                                ]
                            },
                            {
                                "type": "number", "minimum": 0
                            }
                        ]
                    },
                    {
                        "type": "number","minimum": 0
                    }
                    ]
                  }
                }
              }
    }
   \end{json}
    \caption[Test]{Example of schema (done to be compatible for \url{https://python-jsonschema.readthedocs.io/en/stable/})}
 \label{lst:test3}
 \end{listing}
\end{center}





\section{Wide column stores}
\subsection{Can you explain the limitations of the traditional relational model?}

\begin{itemize}
    \item Doesn't scale to big data: Operations such as JOIN are very expensive on large datasets
    \item Limitation to a single machine: Performing classical RDBMS operations across a cluster infrastructure is very hard, as transactions need to be synchronized across nodes
\end{itemize}

\subsection{Can you explain the differences and similarities between a wide column store and the traditional relational model?}

In wide column stores we store together what is accessed together. The traditional relational model (Boyce-Codd) features expensive koin operations when data is retrieved. In wide column stores we denormalize the data such that data is stored in one single table and access operations are cheaper. 


\subsection{Can you explain why wide column stores are called wide column stores, in particular on the storage level?}

As the column name is not fixed wide column stores can have a very large number of columns. Groups of columns are called regions, a row-subset of these are stored together. 

\subsection{Do you know and can you contrast the two ways data can be distributed (partitioning, replicating)?}

\begin{itemize}
    \item \textbf{Replication}: Same schema and data on multiple nodes of a cluster. 
    \begin{itemize}
        \item High data availability
        \item High read and write speeds
        \item Load-balancing for scalability and throughput
    \end{itemize}
    \item \textbf{Partitioning}: Limitation on how much data can be stored on one node. Multiple nodes only have part of the data. 
    \begin{itemize}
        \item Queries can be executed on parallel on the different partitions (shards) on different nodes. 
    \end{itemize}
\end{itemize}


\subsection{Can you explain the data model behind wide column stores, in particular, rows,columns, column families and cells?}

\begin{itemize}
\item \textbf{Rows}: Have a "Row ID"; thus setup like key-value store 
\item \textbf{Columns}: Columns are not fixed like in an RDBMS. They can be added on the fly if a new entry is added. 
\item \textbf{Column families}: Groups of columns which are stored together; these can not be added on the fly and must be known in advance. 
\item \textbf{Cells}: Format and name of a cell can vary from row to row
\end{itemize}

\textbf{A wide-column store can be interpreted as a two-dimensional key–value store.}

\subsection{Can you explain the motivation behind data getting denormalized into more columnfamilies?}

A set of rows within a columnfamiliy is stored together physically. If we wanna split up the data then we can split up the column families. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/storetog.png}
\end{figure}

\subsection{Can you explain what aspects of an instance (table) in a column store must be typically known in advance, and which can be changed on the fly?}

Column families must be known in advance but columns can be added on the fly.

\subsection{Can you name a few big players, in particular the one behind the initial founding paper?}

Google (BigTable), Apache (HBASE) and Cassandra

\subsection{Can you explain why HBase is based on HDFS, yet low-latency?}

A region (number of rows) and a column familiy of HBase are stored in a sorted key-value pair list on HDFS. These map back to the original HBase "table"

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/hfile_map.png}
\end{figure}

We have a speed increase due to in-memory storage in the MemStore. The MemStore is flushed to HDFS which is lexiographically sorted and thus allows fast access. 

\subsection{Can you explain what regions are for wide column stores?}

{Regions are groups of of rows including "Min-incl." and excluding "Max-excl."}

\subsection{Do you know how to identify a region based on the content of the wide column store?}

HBase Tables are divided horizontally by row key range into “Regions.” A region contains all rows in the table between the region’s start key and end key. Regions are assigned to the nodes in the cluster, called “Region Servers,” and these serve data for reads and writes. A region server can serve about 1,000 regions.


\subsection{Do you know the four basic kinds of (low-level) queries in HBase?}

\begin{itemize}
    \item \textbf{GET}: Get a row
    \item \textbf{PUT}: Add a row
    \item \textbf{SCAN}: Return multiple rows given a start and end row
    \item \textbf{DELETE}: Remove a row
\end{itemize}

\subsection{Can you describe the physical architecture of a wide column store like HBase, and compare it with that of a distributed file system like HDFS?}

\begin{itemize}
    \item \textbf{HDFS}
    
    \includegraphics[width=\linewidth]{img/hdfsov.png}
    \item \textbf{HBase}
    
    \includegraphics[width=\linewidth]{img/hbaseov.png}
\end{itemize}

\begin{table}[H]
\begin{tabular}{|l|l|}
\hline
\textbf{HDFS}                                                                                                                                   & \textbf{HBase}                                                                                                                  \\ \hline
\begin{tabular}[c]{@{}l@{}}HDFS is a Java-based file system utilized \\ for storing large data sets.\end{tabular}                               & \begin{tabular}[c]{@{}l@{}}HBase is a Java based Not \\ Only SQL database\end{tabular}                                          \\ \hline
\begin{tabular}[c]{@{}l@{}}HDFS has a rigid architecture that does not \\ allow changes. It doesn’t facilitate dynamic\\  storage.\end{tabular} & \begin{tabular}[c]{@{}l@{}}HBase allows for dynamic \\ changes and can be utilized for \\ standalone applications.\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}HDFS is ideally suited for write-once \\ and read-many times use cases\end{tabular}                                  & \begin{tabular}[c]{@{}l@{}}HBase is ideally suited for \\ random write and read of data \\ that is stored in HDFS.\end{tabular} \\ \hline
\end{tabular}
\end{table}


\textbf{Components}

\begin{itemize}
    \item \textbf{HMaster}: Create and delete table operations; assigns regions to region servers and splits regions if necessary; handles fail overs
    \item \textbf{Regionserver}: Handles the storage of the regions 
\end{itemize}

\subsection{Do you know the physical layers of HBase (table, region, Store, Memstore, HFile,HBase block, Key Value)?}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{img/ondisk_hbase.png}
\end{figure}

\begin{itemize}
    \item \textbf{Table}: Consists of rows with a key and multiple column families
    \item \textbf{Region}: {Regions are groups of of rows including "Min-incl." and excluding "Max-excl."}
    \item \textbf{Store}: Column family and a set of rows that are stored together. 
    \item \textbf{Memstore}: New data is first written to the MemStore; as soon as the MemStore is full it is flushed to a HFile
    \item \textbf{HFile}: File containing multpile HBlocks which store the data in an sorted and immutable fashion. 
    \item \textbf{HBlock}: Number of KeyValues that get read at a time. (64 kb) On data lookup the key is looked up in the index file which contains a number of keys and the correpoding location in the HFile.
    \item \textbf{KeyValue}
        \includegraphics[width=0.5\linewidth]{img/hfile_keyvalue.png}
\end{itemize}

\subsection{Do you know the difference between an HDFS block and an HBase block as well astheir typical sizes?}

\begin{itemize}
    \item HBase blocks are the unit of indexing (as well as caching and compression) in HBase and allow for fast random access
    \item HDFS blocks are the unit of the file system distribution and data locality
\end{itemize}

Good StackOverflow thread on the topic https://stackoverflow.com/questions/12472655/random-access-performance-in-hbase-block-size-in-hdfs. 

\subsection{Can you explain how new cells are written to HBase via the cell store?}

A new cell of data is written to the MemStore (Per Region and Family), as soon as the MemStore is full it is flushed to the disk in form of a new HFile. Upon flush mergesort is performed. 

\subsection{Can you explain how cells are read from HBase via both the stored HFiles and the Memstore?}

If a cell is read the MemStore and the different HFiles are searched. In the HFiles only a subset of the file needs to be seqrched due to the sorting. 

\subsection{Can you explain what compaction and flushing is?}

Flusing is the process of creating a HFile from the MemStore. During compaction multiple HBlocks are merged and sorted. 

\subsection{Can you sketch the contents of an HFile and know what it corresponds to in an HBase table?}

See picture above. 

\subsection{Can you explain why it is crucial not to have too long column family names?}

Keep your column family names as short as possible. The column family names are stored for every value (ignoring prefix encoding). 

\subsection{Can you populate data into, and execute queries on top of HBase?}



\section{Data models}
\subsection{Can you explain the difference between syntax and a data model?}

The data model is the manifestation of the data from a logical standpoint, whereas syntax is the manifestation of the data from a physical standpoint. E.g. a JSON string could potentially be modelled in approximately the same way in XML, only the syntax will obviously differ.

One such difference is that the labels of nodes are on edges for JSON, whereas they are on te nodes themselves in XML.

\subsection{Can you explain why and how trees can be used to model data that is denormalized(i.e., not in first normal form)?}

Why? Trees can represent nested structure of data, thereby avoiding duplicates. How? JSON and XML contain data types which can be recursively defined to enable the representation of nested structure. a valid JSON string can be the value of a key in another JSON string.

\subsection{Can you relate syntaxes to data models (e.g., CSV to tables, XML/JSON to trees, ...)?}

Answer is in the question.

\subsection{Can you explain why trees modeling XML (infoset) have labels on the nodes, while trees modeling JSON have labels on edges?}

Because they are element names and keys, respectively.

\subsection{Can you name a few data models for XML?}

(Infoset, PSVI, XDM)

\subsection{Are you able, given an XML document, to sketch a tree representing that data (with a document node, elements, attributes, text, ...)?}

See Figure~\ref{fig:treefromdoc}.

\begin{figure}
  \centering
  \includegraphics[width=.8\textwidth]{img/treefromdoc.png}
  \caption{Tree from document.}
  \label{fig:treefromdoc}
\end{figure}

\subsection{Do you know the different between an atomic type/value, and a structured type/value (regardless of the exact data model)?}

Atomic: cannot be divided into other atomic types, structured type can.

\subsection{Can you name a few atomic types found across a broad number of data models?}

Strings, numbers, booleans, dates and times, tieme intervals, binaries, null.

\subsection{Do you know the difference between the lexical space and the value space of anatomic type?}

Value space is the range of values for a given type, while lexical space is the range of representations.

\subsection{Can you tell the difference between structured types based on lists (e.g., JSON array) and maps (e.g., JSON object)?}

Maps: key value model, whereas lists don't have such a model. Difference is also in syntax: [] vs. \{\} for arrays and objects, respectively.

\subsection{Can you give examples of type cardinalities, and their associated symbols?}

See table~\ref{tab:card}.

\begin{table}
  \centering
  \begin{tabular}{lll}
    \toprule
    How many? & Common sign & Common adjective\\
    \midrule
    One &  & required\\
    Zero or more & / & repeated\\
    Zero or one & ? & optional\\
    One or more & + & \\
    \bottomrule
  \end{tabular}
  \caption{Cardinality}
  \label{tab:card}
\end{table}

\subsection{Can you explain the difference between well-formedness and validity? Do you know what extra information you need to assess validity?}

See section~\ref{diffwellformedvalidity}.


\subsection{Are you able to design simple XML Schemas to validate XML? Can you restrict simpletypes to allow specific values or value fulfilling specific criteria (length, pattern...)? Can you explain how to declare elements with simple types? Can you explain how to declare elements with complex types and various content models (empty, simplecontent, complex content, mixed content)?}

Example XML schema shown in listing~\ref{lst:xmlschema}.


\begin{center}
\begin{listing}[!ht]
  \begin{xml}
  <?xml version="1.0" encoding="UTF-8" ?>
  <xs:schema xmlns:xs="http://www.w3.org/2001/XMLSchema">
    <xs:element name="shiporder">
      <xs:complexType>
        <xs:sequence>
          <xs:element name="orderperson" type="xs:string"/>
          <xs:element name="shipto">
            <xs:complexType>
              <xs:sequence>
                <xs:element name="name" type="xs:string"/>
                <xs:element name="address" type="xs:string"/>
                <xs:element name="city" type="xs:string"/>
                <xs:element name="country" type="xs:string"/>
              </xs:sequence>
            </xs:complexType>
          </xs:element>
          <xs:element maxOccurs="unbounded" name="item">
            <xs:complexType>
              <xs:sequence>
                <xs:element name="title" type="xs:string"/>
                <xs:element minOccurs="0" name="note" type="xs:string"/>
                <xs:element name="quantity" type="xs:positiveInteger"/>
                <xs:element name="price" type="xs:decimal"/>
              </xs:sequence>
            </xs:complexType>
          </xs:element>
        </xs:sequence>
        <xs:attribute name="orderid" type="xs:string" use="required"/>
      </xs:complexType>
    </xs:element>
  </xs:schema>
  \end{xml}
  \caption[Test]{XML Schema example from \href{https://www.w3schools.com/xml/schema_example.asp}{here}}
  \label{lst:xmlschema}
\end{listing}
\end{center}

\subsection{Can you tell, given an XML Schema, whether an XML document is valid against it,with software (oXygen) but also with your own eyes?}

Go through a couple of examples.

\subsection{Are you able to design simple JSON Schemas to validate JSON?}

See Listing~\ref{lst:test3}.

\subsection{Can you name (without details) further data modeling technologies for tree-likedata?}

Avro, protocol buffers, ...

\subsection{Do you understand that a table in first normal form can be seen as a homogeneous collection of flat trees (one per row and all with the same attributes)?}





\section{Massive parallel processing (MapReduce)}
\subsection{Can you explain the patterns that appear in large-scale data processing: map, shuffle?}

\begin{itemize}
    \item \textbf{Map}: each worker node applies the map function to the local data, and writes the output to a temporary storage. A master node ensures that only one copy of the redundant input data is processed.

\item \textbf{Shuffle}: worker nodes redistribute data based on the output keys (produced by the map function), such that all data belonging to one key is located on the same worker node.

\item \textbf{Reduce}: worker nodes now process each group of output data, per key, in parallel.

\end{itemize}

\subsection{Can you explain how the mastery of these patterns can bring significant improvements in performance?}

Many tasks can be performed in a distributed and parallel fashion such that the work is shared across workers. 

\subsection{Can you explain the MapReduce model, also in terms of the patterns depicted above (map, shuffle, reduce)?}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/map_reduce_ex.png}
\end{figure}


\subsection{Can you describe the physical architecture of MapReduce (map tasks, reduce tasks as well as data flow)? Can you explain version 1 of the architecture (JobTracker, TaskTrackers) and its limits?}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{img/mapred_v1.png}
\end{figure}

\begin{itemize}
    \item \textbf{JobTracker}: Runs centrally; recieves MapReduce request from the client and requests data locations from the Namenode. JobTracker finds the best TaskTracker nodes to execute tasks based on the data locality (proximity of the data) and the available slots to execute a task on a given node. JobTracker monitors the individual TaskTrackers and the submits back the overall status of the job back to the client.
    \item \textbf{TaskTracker}: Runs on the datanodes and executes Mapper and Reducer tasks. In communication with the JobTracker, signalling the progress of the execution. 
\end{itemize}


\subsection{Can you outline what a map function looks like, and what a reduce function looks like?}

\begin{itemize}
    \item \textbf{MAP} The Map function takes a series of key/value pairs, processes each, and generates zero or more output key/value pairs. The input and output types of the map can be (and often are) different from each other. If the application is doing a word count, the map function would break the line into words and output a key/value pair for each word. Each output pair would contain the word as the key and the number of instances of that word in the line as the value.
    \item \textbf{REDUCE} The framework calls the application's Reduce function once for each unique key in the sorted order. The Reduce can iterate through the values that are associated with that key and produce zero or more outputs. In the word count example, the Reduce function takes the input values, sums them and generates a single output of the word and the final sum.
\end{itemize}

\subsection{Do you know the main input and output types of MapReduce?}

The input and output to each phase are key-value pairs.

\subsection{Can you explain how combining improve MapReduce's performance?}

Modules such as the Combiner function can help to reduce the amount of data written to disk, and transmitted over the network when data is shuffled or otherwise copied. 

\subsection{Can you state the assumptions behind reusing the reduce function as a combine function?}

Often the combine and reduce functions are identical. But there are assumptions that must hold. 

\begin{itemize}
    \item Key-value types must be identical for input and output. 
    \item Reduce function must be commutative (items can be exchanged without changing the result) and associative (Order of operations does not matter. If key-value pair 1 and 2 or 2 and 3 are first combined doesn't matter.). 
\end{itemize}

\subsection{Do you understand, in some simple cases, how to design a combine function that would make a MapReduce job faster, even if the combine function is not the exact same as the reduce function? (Example: computing an average, which requires keeping track of the weights in the output of the combine function).}



\subsection{Can you explain why MapReduce shines on top of a distributed file system: "Bring the query to the data"?}

The TaskTracker runs directly on the DataNode thus the query can be executed locally where the data is actually stored. It is cheaper to run the query locally than to bring the copy the data to a central node. After reduction the data can be copied more efficiently. 

\subsection{Can you explain why MapReduce especially makes sense when the bottleneck is the speed of reading and writing data from the disk (as opposed to other bottlenecks such as storage capacity or CPU usage)?}

Reading and writing happens largely locally in a distributed fashion where the data is actually processed. 

\subsection{Can you explain how MapReduce splits differ from HDFS blocks, what impedance mismatches arise and how they are addressed?}

MapReduce data splits largely correspond to the physical level HDFS blocks. But as HDFS blocks are just bits of a defined size some data (e.g. key-value pair) may overlap two blocks. When this happens the overlapping data needs to be copied by remote read. 

\subsection{Do you know what the Java API of MapReduce looks like on a high level (Version 2 of the API that we covered, not to be confused with Version 2 of MapReduce running on YARN)?}


\begin{center}
\begin{listing}[H]
  \begin{java}
    // Create a new Job
    Job job = new Job(new Configuration());
    job.setJarByClass(MyJob.class);
    
    // Specify various job-specific parameters
    job.setJobName("myjob");
    job.setInputPath(new Path("in"));
    job.setOutputPath(new Path("out"));
    
    job.setMapperClass(MyJob.MyMapper.class);
    job.setReducerClass(MyJob.MyReducer.class);
    
    // Submit the job, then poll for progress until the job is complete
    job.waitForCompletion(true);
  \end{java}
\end{listing}
\end{center}


\section{Resource Management}



\subsection{Can you explain how YARN works, and how it can be used to improve MapReduce, and to support other technologies like Spark?}

The responsibilities of the job tracker in mapreduce are manifold:

\begin{enumerate}
\item Resource management
\item Scheduling
\item Monitoring
\item Job lifecycle
\item Fault-tolerance
\end{enumerate}


YARN solves multiple issues that arise with vanilla mapreduce and DAG processing:
\begin{enumerate}
\item Scalability
\item The jobtracker bottleneck
\item the fact that the jobtracker has to take care of both scheduling and monitoring.
\item The fixed size and static allocation of jobs.
\item Fungibility, i.e. the ability of a good or asset to be interchanged with other individual goods or assets of the same type.
\end{enumerate}

YARN divides scheduling from monitoring.

\subsection{Can you describe the YARN components?}

\begin{itemize}
\item Resource Manager: does not monitor tasks or restart upon failure.
\item Node Manager
\item Container
\end{itemize}

\subsection{Do you know what a ResourceManager does?}
Pure scheduling
\subsection{Do you know what a NodeManager does?}
Conceptually, the NodeManager is more of a generic and efficient version of TaskTracker (of Hadoop1 architecture) which is more flexible than TaskTracker. In contrast to fixed number of slots for map and reduce tasks in MRV1, the NodeManager of MRV2 has a number of dynamically created resource containers. There is no hard code split available into Map and Reduce slots as in MRV1. The container refers to a collection of resources such as memory, CPU, disk and network IO. The number of containers on a node is the product of configuration parameter and the total amount of node resources. Node manager is the slave daemon of Yarn.

\subsection{Do you know what and where a Container is?}
As previously described, ResourceManager (RM) is the master that arbitrates all the available cluster resources and thus helps manage the distributed applications running on the YARN system. It works together with the per-node NodeManagers (NMs) and the per-application ApplicationMasters (AMs).

\subsection{Do you know what an ApplicationMaster is and does?}
ApplicationMasters are responsible for negotiating resources with the ResourceManager and for working with the NodeManagers to start the containers.

\subsection{Can you list the main resources that are managed in a cluster? .}
Disk storage,memory, CPU, network I/O: all are dynamically allocated for each application.

\subsection{Can you explain, in simpler words, what the added value of YARN is? Can you explainwhat it is an improvement over the first version of MapReduce, which was taking care of resource management on its own, and had issues with this?}

Essentially, YARN allows for a more flexible and dynamic allocation of tasks on a cluster, which also allows to prioritize certain tasks and users while wasting a minimum of space.\\

\textbf{From HADOOP website}\\

The fundamental idea of YARN is to split up the functionalities of resource management and job scheduling/monitoring into separate daemons. The idea is to have a global ResourceManager (RM) and per-application ApplicationMaster (AM). An application is either a single job or a DAG of jobs.

The ResourceManager and the NodeManager form the data-computation framework. The ResourceManager is the ultimate authority that arbitrates resources among all the applications in the system. The NodeManager is the per-machine framework agent who is responsible for containers, monitoring their resource usage (cpu, memory, disk, network) and reporting the same to the ResourceManager/Scheduler.

The per-application ApplicationMaster is, in effect, a framework specific library and is tasked with negotiating resources from the ResourceManager and working with the NodeManager(s) to execute and monitor the tasks.

The ResourceManager has two main components: Scheduler and ApplicationsManager.

The Scheduler is responsible for allocating resources to the various running applications subject to familiar constraints of capacities, queues etc. The Scheduler is pure scheduler in the sense that it performs no monitoring or tracking of status for the application. Also, it offers no guarantees about restarting failed tasks either due to application failure or hardware failures. The Scheduler performs its scheduling function based the resource requirements of the applications; it does so based on the abstract notion of a resource Container which incorporates elements such as memory, cpu, disk, network etc.

The Scheduler has a pluggable policy which is responsible for partitioning the cluster resources among the various queues, applications etc. The current schedulers such as the CapacityScheduler and the FairScheduler would be some examples of plug-ins.

The ApplicationsManager is responsible for accepting job-submissions, negotiating the first container for executing the application specific ApplicationMaster and provides the service for restarting the ApplicationMaster container on failure. The per-application ApplicationMaster has the responsibility of negotiating appropriate resource containers from the Scheduler, tracking their status and monitoring for progress.

MapReduce in hadoop-2.x maintains API compatibility with previous stable release (hadoop-1.x). This means that all MapReduce jobs should still run unchanged on top of YARN with just a recompile.


\subsection{What are the pitfalls and advantages of various types of scheduling?}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{img/fifo.png}
  \caption{FIFO: least efficient, whole cluster devoted to task.}
  \label{fig:fifo}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{img/capacity_scheduling.png}
  \caption{Capacity scheduling increases the amount of queues but does not allow optimal resource allocation.}
  \label{fig:capsched}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{img/fair.png}
  \caption{FAIR scheduling, most optimal scheduling of resources.}
  \label{fig:fairsched}
\end{figure}



\section{Massive parallel processing (Spark)}
\subsection{Can you explain how Spark is more powerful than MapReduce on a data model level?}

The MapReduce network has a very specific structure but YARN supports more general graph topologies; Spark uses the full potential of such more complex graphs. 

\subsection{Can you explain what a Resilient Distributed Dataset (RDD) is?}

Resilient Distributed Dataset (RDD) is a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.

In MapReduce there is substantial I/O between intermediate operations. In Spark intermediate results can be stored in distributed memory insted of stable storage thereby increasing speed. (Spillout to disk if necessary)

\subsection{Do you know the difference between an action and a transformation?}

\begin{itemize}
    \item \textbf{Transformation}: Operations that transform the RDD from one form into another. Such operations include \texttt{map}, \texttt{filter} and many more. When transformations are applied on any RDD it will not perform the operation immediately. It will create a DAG (Directed Acyclic Graph) using the applied operation, source RDD and function used for transformation. And it will keep on building this graph using the references until an action is performed. 
    \item \textbf{Action}: As soon as an Action is performed the the transformations are executed and a non-RDD result is returned. These include \texttt{reduce}, \texttt{collect}, \texttt{first} and many others. 
\end{itemize}


\subsection{Do you know the main actions and transformations available in Spark?}

\textbf{Transformations}

\begin{itemize}
    \item \textbf{filter}: Returns a subset of the data based on some condition.
    \item \textbf{map}: Apply a function to each element,
    \item \textbf{flatMap}: Similar to map but returns multiple elements. 
    \item \textbf{distinct}: Removes duplicate elements. 
    \item \textbf{sample}: Return a random subset of elements. 
    \item \textbf{union}: Combine elements of two datasets.
    \item \textbf{intersection}: Return elements found in both datasets. 
    \item \textbf{subtract}: Return elements found in one but not the other datset. 
    \item \textbf{cartesion}: Return a tuple of all pairs. 
\end{itemize}

\textbf{Transformations}

\begin{itemize}
    \item \textbf{collect}: Return all the elements of the dataset as an array at the driver program.
    \item \textbf{count}: Return the number of elements. 
    \item \textbf{countByKey}: Returns counts by keys. 
    \item \textbf{take}: Return first n elements from the dataset. 
    \item \textbf{top}: Return the top n elements from the dataset.
    \item \textbf{takeSample}: Return sample of n elements. 
    \item \textbf{reduce}: e.g. Sum up all the elements and return the sum. 
\end{itemize}

\subsection{Are you able to classify transformations and actions in various buckets (those that work on any value, those that work on key-value pairs, unary binary transformations, ...)?}

\subsubsection{Pair RDDs}

\textbf{Transformations}

\begin{itemize}
    \item \textbf{keys}: Return all keys. 
    \item \textbf{values}: Return all values. 
    \item \textbf{reduceByKey}: Reduce elements by common keys. 
    \item \textbf{groupByKey}: Group elements with common key. 
    \item \textbf{sortByKey}: Sort elements by key. 
    \item \textbf{mapValues}: Apply function to values. 
    \item \textbf{join}: Join by common key. 
    \item \textbf{subtractByKey}: Subtract elements from each other by keys. 
\end{itemize}

\textbf{Actions}

\begin{itemize}
    \item \textbf{countByKey}: Count elements by key. 
    \item \textbf{lookup}: Find element with specific key. 
\end{itemize}

\subsection{Can you describe how transformations run physically (tasks, stages...)?}

Per default there is one task per HDFS block. Tasks are distibuted over the cores of the executors. A stage is a sequence of parallelizable transformations. 

\subsection{Can you explain when and how a series of transformations can be optimized by keeping the same set of machines with no network communication between the transformations?}

When each partition at the parent RDD is used by at most one partition of the child RDD, then we have a narrow dependency. Computations of transformations with this kind of dependency are rather fast as they do not require any data shuffling over the cluster network. In addition, optimizations such as pipelining are also possible.

\subsection{Can you explain what a stage is? Can you relate it to transformations? To tasks? To Jobs?}

Spark stages are the physical unit of execution for the computation of multiple \textbf{tasks}. The Spark stages are controlled by the Directed Acyclic Graph (DAG). There are mainly two stages associated with the Spark frameworks such as, ShuffleMapStage and ResultStage. The ShuffleMapStage is the intermediate phase for the tasks which prepares data for subsequent stages, whereas ResultStage is a final step to the spark function for the particular set of tasks in the spark job.

\textbf{Transformations}\\

\begin{itemize}
    \item \textbf{Narrow Transformations}: These are transformations that do not require the process of shuffling. These actions can be executed in a single stage. Example: map() and filter()
    \item \textbf{Wide Transformations}: These are transformations that require shuffling across various partitions. Hence it requires different stages to be created for communication across different partitions. Example: ReduceByKey
\end{itemize}

\textbf{Jobs} are divided into "stages" based on the shuffle boundary.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/spark_terminology.png}
\end{figure}

\subsection{Can you tell why and when shuffling is needed? In other words, can say whether a transformation has a narrow dependency or a wide dependency?}

\begin{itemize}
    \item \textbf{Narrow Transformations}: These are transformations that do not require the process of shuffling. These actions can be executed in a single stage. Example: map() and filter()
    \item \textbf{Wide Transformations}: These are transformations that require shuffling across various partitions. Hence it requires different stages to be created for communication across different partitions. Example: ReduceByKey
\end{itemize}

\subsection{Can you easily draw a directly acyclic graph for a Spark job, and mark the stages?}

\includegraphics[width=\linewidth]{img/stages.png}

\subsection{Do you understand why keeping an RDD persisted can be useful and improve performance?}

One of the most important capabilities in Spark is persisting (or caching) a dataset in memory across operations. When you persist an RDD, each node stores any partitions of it that it computes in memory and reuses them in other actions on that dataset (or datasets derived from it). This allows future actions to be much faster (often by more than 10x). Caching is a key tool for iterative algorithms and fast interactive use.

\subsection{Can you explain how controlling the way data is partitioned can make execution faster because we can influence stages?}

We try to avoid wide dependencies by making sure that key-values with the same keys are already on the same machine thus reducing shuffling operations. 

\subsection{Can you explain what a DataFrame is and what its benefits are?}

A DataFrame is a distributed collection of data, which is organized into named columns. Conceptually, it is equivalent to relational tables with good optimization techniques.

\begin{itemize}
    \item DataFrames have a smaller memory footprint than "raw" Spark. 
    \item Spark can perform schema inference on various input datasforms, providing a DataFrame with appropriate types
    \item Conceptually similar to a relational database; allow SQL queries. 
    \item Scales from kilobytes to petabytes. 
    \item APIs are available in various programming languages. 
\end{itemize}


\subsection{Can you describe the limitations of DataFrames?}

\begin{itemize}
    \item The compiler is not able to catch errors as the code refers to data attribute names. Errors are detected during the run time after the creation of query plans.
    \item It works better with Scala and very limited with Java.
    \item Domain objects cannot be regenerated from it.
\end{itemize}

\subsection{Can you write queries on DataFrames, both in Spark SQL or using DataFrames transformations?}

\textbf{Spark SQL}

\begin{center}
\begin{listing}[H]
  \begin{python}
    df = spark.read.json('hdfs:///dataset.json')
    df.createOrReplaceTempView("dataset")
    df2 = df.sql("SELECT * FROM dataset " 
                 "WHERE guess = target "
                 "ORDER BY target ASC, country DESC, date DESC") 
    result = df2.take(10)
  \end{python}
\end{listing}
\end{center}

\textbf{Logical Transformations}

\begin{center}
\begin{listing}[H]
  \begin{python}
    df = spark.read.json('hdfs:///dataset.json') 
    df2 = df.filter(df['name'] = 'Einstein') 
    df3 = df.sortBy(asc("theory"), desc("date")) 
    df4 = df.select('year')
    result = df4.take(10)
  \end{python}
\end{listing}
\end{center}




\section{Document stores}
\subsection{Do you understand how collections in documents store generalize the concept of a relational table?}

Yes, essentially document stores generalize the idea of a relational table by encompassing trees. A document store is essentially a collection of trees, and a relational table is essentially a special case of document where the tree structure is flat.

\subsection{Can you explain what documents store can do that relational databases cannot (e.g., heterogeneous collections, schema-less collections, data denormalized into trees...)?}

It can:
\begin{itemize}
\item It can collect and store heterogeneous collections of unstructured or semi structured data.
\item It also does not require normalized data
\item It does not require a schema to collect the data.
\end{itemize}

\subsection{Do you know how to issue MongoDB queries on a low level? Do you know the parameters of the find() function? (query, then projection, then sorting)?}

Couple of examples:
\begin{itemize}
\item The following example returns all fields from all documents in the inventory collection where the status equals "A": \verb|db.inventory.find( { status: "A" } )|
\item Sorting: \verb|db.orders.find().sort( { amount: -1 } )|
\end{itemize}

Further examples shown in Figure~\ref{fig:queryresult}. Note subtleties in~\ref{fig:nestedness}.

\begin{figure}
  \centering
  \includegraphics[width=.5\textwidth]{img/read_statement_mongodb.png}
  \caption{Find query result.}
  \label{fig:queryresult}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.45\textwidth]{img/nestedness_1.png}
  \hfill
  \includegraphics[width=.45\textwidth]{img/nestedness_2.png}
  \caption{Nestedness}
  \label{fig:nestedness}
\end{figure}
\subsection{What is the one operations document stores cannot do that RDBMS can and vice versa? }

Document stores cannot perform join operations, RDBMS don't validate the data once populated.

\subsection{Can you explain how, in a document store, the documents can be sharded and replicated?}

From MongoDB on sharding:
Sharding is a method for distributing data across multiple machines. MongoDB uses sharding to support deployments with very large data sets and high throughput operations.

Database systems with large data sets or high throughput applications can challenge the capacity of a single server. For example, high query rates can exhaust the CPU capacity of the server. Working set sizes larger than the system’s RAM stress the I/O capacity of disk drives.

There are two methods for addressing system growth: vertical and horizontal scaling.

Vertical Scaling involves increasing the capacity of a single server, such as using a more powerful CPU, adding more RAM, or increasing the amount of storage space. Limitations in available technology may restrict a single machine from being sufficiently powerful for a given workload. Additionally, Cloud-based providers have hard ceilings based on available hardware configurations. As a result, there is a practical maximum for vertical scaling.

Horizontal Scaling involves dividing the system dataset and load over multiple servers, adding additional servers to increase capacity as required. While the overall speed or capacity of a single machine may not be high, each machine handles a subset of the overall workload, potentially providing better efficiency than a single high-speed high-capacity server. Expanding the capacity of the deployment only requires adding additional servers as needed, which can be a lower overall cost than high-end hardware for a single machine. The trade off is increased complexity in infrastructure and maintenance for the deployment.

For replication: often a primary copy of the data is kept, and updated first. Then, secondary members of a replica sets are updated. Each shard contains a replica sets, composed of primary vs. secondary copies of the data.


\subsection{Do you understand how indices can make queries faster, like in relational databases?}

Hash indices are a fast way of performing selections of data in a document store. It's essentially a table containing the value and the row id of the document.

\subsection{What are some of the limitations of hash indices?}

\begin{itemize}
\item No support for range queries
\item Hash function not perfect in real life
\item Space requirements for collision avoidance. (pigeonhole principle).
\end{itemize}

\subsection{Do you know what kinds of indices there are (hash, B-trees)?}

Hash tables are tables as described two questions earlier. B trees are trees are balanced trees which result in block access of the data. Some properties of these trees:
\begin{itemize}
\item All leaves are at the same depth.
\item All non-leaf nodes have between 3 and 5 children.
\item The values are only at the leaves.
\end{itemize}

Example shown in Figure

\begin{figure}
  \centering
  \includegraphics[width=.7\textwidth]{img/index_tree.png}
  \caption{Tree as index.}
  \label{fig:tree}
\end{figure}

\subsection{Are you capable of telling if an index is useful to a given query, for simple settings (for example, an index on a single field and a query that selects on that field)?}

Yes, essentially if the created index is used to do the bulk of the primary filtering of a query it's useful. Some post filtering can be done once fetched data is in memory.

\subsection{Do you know that a compound index (e.g., on keys a and b) can also be used as an index on any prefix of the compound key (e.g., on key a only) "for free"?}

Yes, similar to pd.MultiIndex.



\section{Querying trees}
\subsection{Can you explain why a language such as JSONiq provides, in the context of document stores, exactly the same functionality as SQL in a relational database?}

Every language construct in JSONiq is an expression; they are fully composable and the result of a query is the result of the evaluation of its main expression. Project, Select, Filter, Join, Group, Order... Like SQL, JSONiq can do all that.

\subsection{Can you name the first-class citizen of the JDM (sequence of items)?}

The sequence of items is the first-class citizen and has the following properties. 
 
\textbf{Properties} 
\begin{itemize}
    \item Heterogeneous
    \item Denormalized
\end{itemize}

\subsection{Can you name various kinds of items in the JDM?}

\textbf{JSONiq Data Model (JDM): Items}

\begin{itemize}
    \item Atomic
    \item Object 
    \item Array 
    \item Function 
\end{itemize}

\subsection{Can you name a few query languages in the XML/JSON ecosystem?}

XQuery (XML/JSON), JSON QUery, JSONIq

\subsection{Are you able, in JSONiq, to construct items (atomic items, elements, etc.)?}

\begin{itemize}
    \item \textbf{Strings}: "foo"; Escaping \\" or \\u0022
    \item \textbf{Numbers}: 42 (int), 3.14 (decimal), -6.022E23 (double); Casting: int("123")
    \item \textbf{Booleans}: true
    \item \textbf{Dates and Time}: date("2013-05-01Z"), dateTime("2013-06-21T05:00:00"), time("05:00:00-08:00"), etc.
    \item \textbf{Binaries}: hexBinary("0CD7"), base64Binary("RGFzc2Vs")
    \item \textbf{JSON Objects}: JSON-notation
    \item \textbf{Sequences}: Comma-separated
    \item \textbf{Ranges}: 1 to 100
\end{itemize}

\subsection{Are you able, in JSONiq, to perform logical operations? Do you understand what the Effective Boolean Value of a sequence and how it relates to logical operations?}

eg. \texttt{and}, \texttt{or} and \texttt{not}

These are the effective Boolean values:

\begin{itemize}
    \item False if the operand is an empty sequence or a Boolean false.
    \item Otherwise, the value is true.
\end{itemize}


\subsection{Are you able, in JSONiq, to perform arithmetic operations (addition, etc.)? Do you understand the constraints on the input sequences of such operations? Can you explain the behavior of these operations on empty sequences? Can you explain what happens if one of the two operands is a node and not an atomic item?}

\begin{itemize}
    \item \textbf{+}: Addition
    \item \textbf{-}: Subtraction
    \item \textbf{*}: Multiplication
    \item \textbf{div}: Division
    \item \textbf{idiv}: Integer division
    \item \textbf{mod}: modulo
\end{itemize}

\subsection{Are you able, in JSONiq, to perform comparisons (lt, ge, etc.)? Do you understand the constraints on the input sequences of such operations? Can you explain the behavior of these operations on empty sequences? Can you explain what happens if one of the two operands is a node and not an atomic item?}

The types have to match for such an operation to work. The comparison operator between an empty sequence and any other value will return an empty sequence. 

\begin{table}[H]
\begin{tabular}{|l|l|}
\hline
equality     & 1 + 1 eq 2                                                         \\ \hline
inequality   & 6 * 7 ne 21 * 2                                                    \\ \hline
greater than & \begin{tabular}[c]{@{}l@{}}234 gt 123\\ 234 ge 123\end{tabular}    \\ \hline
less than    & \begin{tabular}[c]{@{}l@{}}42.3 lt 7.2 \\ 42.3 le 7.2\end{tabular} \\ \hline
\end{tabular}
\end{table}


\subsection{Do you understand how general comparisons (<, >=, etc.) work on sequences with more than one item, and implicitly use an existential quantifier)?}

Sequence operations work will yield true if one of the comparisons returns true.\\

\textbf{Universal Quantifier}\\
\texttt{every $i in 1 to 10 satisfies $i gt 0}

\textbf{Existential Quantifier}\\
\texttt{some $i in -5 to 5, $j in 1 to 10 satisfies $i eq $j}

\subsection{Do you understand how FLWOR expressions work and describe what they return? (for clause, let clause, where clause, order by clause, etc.)}

FLWOR expressions correspond to SQL's SELECT-FROM-WHERE statements, but they are more general and more flexible. In particular, clauses can almost appear in any order (apart that it must begin with a for or let clause, and end with a return clause).

\begin{verbatim}
    for $x in collection("captains")
    return $x.name
\end{verbatim}

\textbf{Join}

\begin{verbatim}
    for $captain in collection("captains"), $movie in collection("movies")[ try { $$.captain eq $captain.name } catch * { false } ]
    return { "captain" : $captain.name, "movie" : $movie.name }
\end{verbatim}

\subsection{Are you able to use further expressions (if-then-else, switch, ...)?}

\begin{verbatim}
    if(count(doc("file.xml")//country) gt 1000) then "Large file!"
    else "Small file."
\end{verbatim}

\begin{verbatim}
    switch($country.code) case "CH" return
        ("gsw", "de", "fr", "it", "rm") case "F" return "fr"
    case "D" return "de"
    case "I" return "it"
    default return "en"
\end{verbatim}

\subsection{Do you understand how to dynamically build JSON content with object and array constructors?}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{img/json_construct.png}
\end{figure}

\subsection{Do you understand that expressions can be combined at will, as any expression takes and returns sequences of items? Do you know how to use parentheses to make precedence clear, like you did in primary school with + and *?}

Use parentheses to override or when in doubt!

\begin{table}[H]
\begin{tabular}{|l|}
\hline
Precedence (low first)                                                                                                                     \\ \hline
Comma                                                                                                                                      \\ \hline
Data Flow (FLWOR, if-then-else, switch...)                                                                                                 \\ \hline
Logic                                                                                                                                      \\ \hline
Comparison                                                                                                                                 \\ \hline
String concatenation                                                                                                                       \\ \hline
Range                                                                                                                                      \\ \hline
Arithmetic                                                                                                                                 \\ \hline
Path expressions                                                                                                                           \\ \hline
Filter predicates, dynamic function calls                                                                                                  \\ \hline
\begin{tabular}[c]{@{}l@{}}Literals, constructors and variables\\ Function calls, named function references, inline functions\end{tabular} \\ \hline
\end{tabular}
\end{table}

\subsection{Do you know the JSONiq type syntax (atomic types taken from XML Schema, syntax for object and array types, as well as cardinality symbols), and how to use typechecking (instance of, cast as, etc.)?}

\begin{verbatim}
    for $x as array in ( 
        [1],
        [2]
        )
    return $x + $x cast as double
\end{verbatim}

\begin{verbatim}
    (3.14,"foo") instance of integer*
\end{verbatim}

\subsection{Given a collection of JSON objects (for example JSON Lines on HDFS), are you able to use Rumble to write JSONiq queries (FLWOR) that do projection? selection? (we leave grouping, join, ... aside in the context of this lecture, however you need to know that this can be done with JSONiq as well).}



%\end{multicols}
\end{document}
% Local Variables:
% TeX-command-extra-options: "-shell-escape"
% End: